# -*- coding: utf-8 -*-
"""DataScience Tutor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fzaNkCHdekaFW9j0mW6xh5DsxKj6zuWk
"""

!pip install streamlit langchain langchain-google-genai google-generativeai

# Commented out IPython magic to ensure Python compatibility.
# %%writefile datasci_tutor.py
# import streamlit as st
# from langchain.memory import ConversationBufferMemory
# from langchain_google_genai import ChatGoogleGenerativeAI
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.output_parsers import StrOutputParser
# 
# # Load API Key
# with open("/content/Key.txt") as f:
#     API_KEY = f.read().strip()
# 
# # Initialize Memory
# if "memory" not in st.session_state:
#     st.session_state.memory = ConversationBufferMemory()
# 
# # Define AI Tutor Prompt
# prompt = ChatPromptTemplate(
#     messages=[
#         (
#             "system",
#             """
#             You are an AI-powered Data Science Tutor.
#             Your job is to help users with data science topics, including:
#             - Machine Learning
#             - Data Analytics
#             - Python for Data Science
#             - Deep Learning
#             - Statistics and Probability
#             - Data Visualization (Matplotlib, Seaborn)
#             - SQL for Data Science
# 
#             Always provide **clear, step-by-step explanations** with examples.
#             If a question is **not related to data science**, politely refuse to answer.
# 
#             Keep responses **short and easy to understand**.
#             """
#         ),
#         ("human", "{question}")
#     ]
# )
# 
# # Initialize AI Model
# chat_model = ChatGoogleGenerativeAI(google_api_key=API_KEY, model="gemini-1.5-flash")
# 
# # Output Parser
# output_parser = StrOutputParser()
# 
# # Processing Chain
# chain = prompt | chat_model | output_parser
# 
# # Function to Get AI Response
# def get_response(question):
#     # Retrieve past chat history
#     chat_history = st.session_state.memory.chat_memory.messages
# 
#     # Get AI-generated response
#     response = chain.invoke({"question": question})
# 
#     # Store conversation in memory
#     st.session_state.memory.save_context({"question": question}, {"response": response})
# 
#     return response
# 
# # Streamlit UI Setup
# st.set_page_config(page_title="AI Data Science Tutor", page_icon="ðŸ“Š", layout="wide")
# 
# # Custom CSS for Styling
# st.markdown(
#     """
#     <style>
#         body {
#             background-color: #f4f4f9;
#         }
#         .centered-heading {
#             text-align: center;
#             font-style: italic;
#             font-size: 36px;
#             color: #1a237e;
#             margin-bottom: 0px;
#         }
#         .centered-subheading {
#             text-align: center;
#             font-size: 20px;
#             color: #555;
#             margin-top: 5px;
#             margin-bottom: 20px;
#         }
#         .chat-human {
#             background-color: #e3f2fd;
#             color: #0d47a1;
#             padding: 8px;
#             border-radius: 8px;
#             margin-bottom: 8px;
#         }
#         .chat-ai {
#             background-color: #e8f5e9;
#             color: #1b5e20;
#             padding: 8px;
#             border-radius: 8px;
#             margin-bottom: 8px;
#         }
#         .sidebar-header {
#             color: #1a237e;
#             font-weight: bold;
#             font-size: 18px;
#         }
#     </style>
#     """,
#     unsafe_allow_html=True
# )
# 
# # Centered Heading and Subheading
# st.markdown('<h1 class="centered-heading">ðŸ“Š AI Data Science Tutor</h1>', unsafe_allow_html=True)
# st.markdown('<h3 class="centered-subheading">Ask any question related to Data Science!</h3>', unsafe_allow_html=True)
# 
# # Sidebar for Chat History
# with st.sidebar:
#     st.markdown('<div class="sidebar-header">ðŸ“œ Chat History</div>', unsafe_allow_html=True)
#     if st.session_state.memory.chat_memory.messages:
#         for msg in st.session_state.memory.chat_memory.messages:
#             if msg.type == "human":
#                 st.markdown(f'<div class="chat-human">ðŸ§‘ **You:** {msg.content}</div>', unsafe_allow_html=True)
#             else:
#                 st.markdown(f'<div class="chat-ai">ðŸ¤– **AI:** {msg.content}</div>', unsafe_allow_html=True)
#     else:
#         st.write("No chat history yet.")
# 
# # User Input
# user_question = st.text_area("Enter your data science question:", placeholder="e.g., What is Gradient Descent?")
# 
# # Fetch and Display Answer
# if st.button("Get Answer", use_container_width=True):
#     if user_question:
#         with st.spinner("Thinking..."):
#             answer = get_response(user_question)
#             st.success("âœ… Hereâ€™s your answer:")
#             st.write(answer)
#     else:
#         st.warning("âš  Please enter a question.")
#

!npm install localtunnel
!streamlit run /content/datasci_tutor.py &>/content/logs.txt &
!npx localtunnel --port 8501 & curl ipv4.icanhazip.com